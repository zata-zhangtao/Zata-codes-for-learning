{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Langchain 中，当你运行一个 LLM、Chain、Agent 或 Tool 时，会经历一系列的步骤（例如，开始调用 LLM，LLM 返回结果，Chain 开始执行，Tool 被调用等）。事件系统（通过回调处理器 CallbackHandler 实现）允许你在这些步骤发生时执行自定义代码。这对于日志记录、监控、调试、数据收集、UI 更新等都非常有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 准备工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-openai in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (0.3.16)\n",
      "Requirement already satisfied: pandas in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (2.2.3)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langchain) (0.3.58)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langchain) (2.11.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langchain-openai) (1.77.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Using cached aiohttp-3.11.18-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Using cached pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached frozenlist-1.6.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached multidict-6.4.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached propcache-0.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached yarl-1.20.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading langchain_community-0.3.23-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached aiohttp-3.11.18-cp311-cp311-macosx_11_0_arm64.whl (457 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached multidict-6.4.3-cp311-cp311-macosx_11_0_arm64.whl (37 kB)\n",
      "Using cached pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "Using cached yarl-1.20.0-cp311-cp311-macosx_11_0_arm64.whl (94 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.6.0-cp311-cp311-macosx_11_0_arm64.whl (122 kB)\n",
      "Using cached propcache-0.3.1-cp311-cp311-macosx_11_0_arm64.whl (45 kB)\n",
      "Installing collected packages: propcache, multidict, httpx-sse, frozenlist, aiohappyeyeballs, yarl, aiosignal, pydantic-settings, aiohttp, langchain-community\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [langchain-community]ngchain-community]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 frozenlist-1.6.0 httpx-sse-0.4.0 langchain-community-0.3.23 multidict-6.4.3 propcache-0.3.1 pydantic-settings-2.9.1 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "# 安装必要的库 (如果还没安装)\n",
    "!pip install langchain langchain-openai pandas langchain-community # pandas 仅用于某些回调数据结构示例\n",
    "# !pip install langsmith # 如果你想体验 LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict, List, Union, Optional, Sequence\n",
    "from uuid import UUID\n",
    "\n",
    "# 设置你的 OpenAI API 密钥 (或者你选择的其他 LLM 提供商的密钥)\n",
    "# 强烈建议从环境变量中读取\n",
    "\n",
    "\n",
    "#这里我使用阿里云的百炼平台\n",
    "\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.callbacks.manager import CallbackManager, AsyncCallbackManager\n",
    "from langchain_core.outputs import LLMResult, ChatResult, GenerationChunk\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 使用内置的StdOutCallbackHandler\n",
    "\n",
    "当你运行下面的代码时，你会看到详细的输出，包括 on_llm_start, on_llm_end 等事件，以及它们携带的数据（如 prompt, response）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LLM Call with StdOutCallbackHandler ---\n",
      "\n",
      "LLM Response:\n",
      "Hello! I'm just a program, so I don't have feelings, but thanks for asking! How can I assist you today?\n",
      "\n",
      "--- LLM Call using 'config' (preferred for LCEL) ---\n",
      "\n",
      "LLM Response (config):\n",
      "Why don't skeletons fight each other?\n",
      "\n",
      "Because they don't have the guts!\n",
      "\n",
      "--- Chain Call with StdOutCallbackHandler (using config) ---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new PromptTemplate chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Chain Response:\n",
      "content='The capital of France is **Paris**. It is the political, cultural, and economic center of the country, known for its iconic landmarks such as the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 15, 'total_tokens': 61, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-e775b54d-9dc0-9808-914c-7d05e2dc6897', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--851f3dfb-5dfb-49a7-8eb1-b5d6535f8289-0' usage_metadata={'input_tokens': 15, 'output_tokens': 46, 'total_tokens': 61, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "# 初始化 LLM\n",
    "llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    model=\"qwen-plus\",  # 此处以qwen-plus为例，您可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# 创建一个 StdOutCallbackHandler 实例\n",
    "stdout_handler = StdOutCallbackHandler()\n",
    "\n",
    "# 使用回调处理器运行 LLM\n",
    "print(\"--- LLM Call with StdOutCallbackHandler ---\")\n",
    "# 方法1: 在构造时传入\n",
    "llm_with_stdout = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    model=\"qwen-plus\",  # 此处以qwen-plus为例，您可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "    temperature=0,\n",
    "    callbacks=[stdout_handler] # 可以传入一个列表\n",
    ")\n",
    "response = llm_with_stdout.invoke(\"Hello, how are you today?\")\n",
    "print(\"\\nLLM Response:\")\n",
    "print(response.content)\n",
    "\n",
    "print(\"\\n--- LLM Call using 'config' (preferred for LCEL) ---\")\n",
    "# 方法2: 使用 invoke/stream/batch 的 config 参数 (更推荐，特别是对于 LCEL)\n",
    "response_config = llm.invoke(\n",
    "    \"Tell me a short joke.\",\n",
    "    config={\"callbacks\": [stdout_handler]}\n",
    ")\n",
    "print(\"\\nLLM Response (config):\")\n",
    "print(response_config.content)\n",
    "\n",
    "# 使用回调处理器运行 Chain\n",
    "prompt = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt) # 旧版 Chain，目前先演示旧版，后面有新版\n",
    "\n",
    "print(\"\\n--- Chain Call with StdOutCallbackHandler (using config) ---\")\n",
    "chain_response = chain.invoke(\n",
    "    {\"country\": \"France\"},\n",
    "    config={\"callbacks\": [stdout_handler]}\n",
    ")\n",
    "print(\"\\nChain Response:\")\n",
    "print(chain_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 创建自定义回调处理器\n",
    "\n",
    "通过继承 BaseCallbackHandler，你可以创建自己的处理器来精确控制如何响应特定的事件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoggerV1] Handler Initialized\n",
      "\n",
      "--- LLM Call with MyCustomHandler ---\n",
      "\n",
      "LLM Response (Custom Handler):\n",
      "Learning **Langchain** offers several benefits, especially for developers and data scientists interested in building applications that leverage large language models (LLMs) like OpenAI's GPT, Anthropic's Claude, or other models. Langchain is a framework designed to simplify the integration of LLMs into applications by providing tools, abstractions, and pre-built components. Here are some key benefits:\n",
      "\n",
      "### 1. **Simplified Integration with LLMs**\n",
      "   - Langchain abstracts away much of the complexity involved in integrating LLMs into applications. Instead of manually handling API calls, input/output formatting, and model management, Langchain provides ready-to-use components that streamline these processes.\n",
      "   - This allows developers to focus on higher-level tasks, such as designing user experiences or optimizing workflows.\n",
      "\n",
      "### 2. **Modular Components**\n",
      "   - Langchain is built around modular components (e.g., prompts, chains, agents, memory), which can be combined in various ways to create custom workflows. This modularity makes it easier to experiment with different configurations and adapt to specific use cases.\n",
      "   - For example, you can easily switch between different LLM providers (OpenAI, Anthropic, Hugging Face, etc.) without changing much of your code.\n",
      "\n",
      "### 3. **Enhanced Prompt Engineering**\n",
      "   - Langchain includes tools for creating and managing complex prompts, which are crucial for guiding LLMs to produce desired outputs. It supports dynamic prompt generation, conditional branching, and multi-step reasoning.\n",
      "   - You can also use templates and variables to create reusable prompts tailored to your application's needs.\n",
      "\n",
      "### 4. **Memory Management**\n",
      "   - One of the standout features of Langchain is its support for **memory**, which allows LLMs to retain context across multiple interactions. This is particularly useful for building conversational agents or chatbots where maintaining context is essential.\n",
      "   - Langchain provides different types of memory (e.g., buffer memory, vector store memory) that can be integrated seamlessly into your application.\n",
      "\n",
      "### 5. **Agent Framework**\n",
      "   - Langchain includes an **agent framework** that enables the creation of autonomous systems capable of performing tasks like web browsing, data retrieval, or interacting with APIs. Agents can chain together multiple actions and make decisions based on intermediate results.\n",
      "   - This is ideal for building AI assistants or automating complex workflows.\n",
      "\n",
      "### 6. **Vector Stores and Retrieval Augmented Generation (RAG)**\n",
      "   - Langchain integrates with vector databases (e.g., Pinecone, Weaviate, FAISS) to enable **Retrieval Augmented Generation (RAG)**, where the LLM retrieves relevant information from a knowledge base before generating a response.\n",
      "   - This improves the accuracy and relevance of responses, especially when working with domain-specific or proprietary data.\n",
      "\n",
      "### 7. **Cross-Platform Compatibility**\n",
      "   - Langchain supports multiple LLM providers and frameworks, making it easy to switch between them or use multiple models simultaneously. This flexibility ensures that you're not locked into a single vendor or technology stack.\n",
      "\n",
      "### 8. **Community and Ecosystem**\n",
      "   - Langchain has a growing community of developers contributing to its ecosystem. This means there are plenty of tutorials, examples, and third-party extensions available to help you get started and expand your projects.\n",
      "   - The active community also contributes to rapid development and improvement of the framework.\n",
      "\n",
      "### 9. **Scalability and Performance Optimization**\n",
      "   - Langchain helps optimize the performance of LLM-based applications by providing tools for caching, batching, and parallel processing. These optimizations can reduce costs and improve response times, especially for high-traffic applications.\n",
      "\n",
      "### 10. **Real-World Applications**\n",
      "   - By learning Langchain, you gain the skills to build real-world applications such as:\n",
      "     - **Chatbots**: Conversational interfaces for customer support, virtual assistants, or educational platforms.\n",
      "     - **Content Generators**: Tools for generating articles, reports, or marketing materials.\n",
      "     - **Code Assistants**: AI-driven coding assistants that suggest code snippets or debug issues.\n",
      "     - **Search Engines**: Enhanced search systems that provide more accurate and context-aware results.\n",
      "     - **Automated Workflows**: Systems that automate repetitive tasks using LLMs.\n",
      "\n",
      "### 11. **Stay Ahead in the AI Landscape**\n",
      "   - As LLMs continue to evolve and become more integrated into various industries, having expertise in frameworks like Langchain positions you at the forefront of this technological shift. It opens up opportunities in fields like natural language processing (NLP), conversational AI, and generative AI.\n",
      "\n",
      "### Conclusion\n",
      "Learning Langchain empowers you to harness the full potential of LLMs in a structured and efficient manner. Whether you're building a simple chatbot or a complex AI-driven system, Langchain provides the tools and flexibility needed to bring your ideas to life. Its modular design, robust features, and strong community support make it an invaluable asset for anyone looking to work with LLMs in a professional or academic setting.\n",
      "\n",
      "--- Chain Call with MyCustomHandler ---\n",
      "\n",
      "[LoggerV1] Chain Start:\n",
      "  Serialized: None\n",
      "  Inputs: {'country': 'Canada'}\n",
      "\n",
      "[LoggerV1] Chat Model Start (ID: fd160a3d-ed95-47ef-a830-38920fa14cc5):\n",
      "  Message Group 1:\n",
      "    - human: What is the capital of Canada?...\n",
      "\n",
      "[LoggerV1] LLM End (ID: fd160a3d-ed95-47ef-a830-38920fa14cc5):\n",
      "  Generation Group 1:\n",
      "    - Text: The capital of Canada is **Ottawa**. It was selected by Queen Victoria in 1857 to serve as a comprom...\n",
      "    - Tool Calls: None\n",
      "  LLM Output (Token Usage, etc.): {'token_usage': {'completion_tokens': 55, 'prompt_tokens': 15, 'total_tokens': 70, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-3db44766-0198-9e7d-b33a-f00ab12a51cc', 'service_tier': None}\n",
      "\n",
      "[LoggerV1] Chain End:\n",
      "  Outputs: {'text': 'The capital of Canada is **Ottawa**. It was selected by Queen Victoria in 1857 to serve as a compromise between Toronto and Montreal, the two largest cities at the time, and it has since become the political and administrative center of the country.'}\n",
      "\n",
      "Chain Response (Custom Handler):\n",
      "{'country': 'Canada', 'text': 'The capital of Canada is **Ottawa**. It was selected by Queen Victoria in 1857 to serve as a compromise between Toronto and Montreal, the two largest cities at the time, and it has since become the political and administrative center of the country.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.outputs import GenerationChunk, ChatGenerationChunk\n",
    "\n",
    "class MyCustomHandler(BaseCallbackHandler):\n",
    "    def __init__(self, description: str = \"MyHandler\"):\n",
    "        self.description = description\n",
    "        print(f\"[{self.description}] Handler Initialized\")\n",
    "\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM starts running.\"\"\"\n",
    "        print(f\"\\n[{self.description}] LLM Start:\")\n",
    "        print(f\"  Serialized: {serialized}\") # 描述 LLM 的信息\n",
    "        print(f\"  Prompts: {prompts}\")\n",
    "\n",
    "    def on_chat_model_start(\n",
    "        self,\n",
    "        serialized: Dict[str, Any],\n",
    "        messages: List[List[BaseMessage]],\n",
    "        *,\n",
    "        run_id: UUID,\n",
    "        parent_run_id: Optional[UUID] = None,\n",
    "        tags: Optional[List[str]] = None,\n",
    "        metadata: Optional[Dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when a Chat Model starts running.\"\"\"\n",
    "        print(f\"\\n[{self.description}] Chat Model Start (ID: {run_id}):\")\n",
    "        # print(f\"  Serialized: {serialized}\") # 通常与 on_llm_start 类似\n",
    "        for i, msg_list in enumerate(messages):\n",
    "            print(f\"  Message Group {i+1}:\")\n",
    "            for msg in msg_list:\n",
    "                print(f\"    - {msg.type}: {msg.content[:80]}...\") # 打印消息类型和部分内容\n",
    "\n",
    "    def on_llm_new_token(\n",
    "        self,\n",
    "        token: str,\n",
    "        *,\n",
    "        chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]] = None, # 注意这里的类型提示变化\n",
    "        run_id: UUID,\n",
    "        parent_run_id: Optional[UUID] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Any:\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "        # print(f\"[{self.description}] New Token: '{token}' (Run ID: {run_id})\")\n",
    "        # 为了避免过多输出，streaming token 通常只在需要时打印\n",
    "        pass\n",
    "\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, *, run_id: UUID, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "        print(f\"\\n[{self.description}] LLM End (ID: {run_id}):\")\n",
    "        for i, generation_list in enumerate(response.generations):\n",
    "            print(f\"  Generation Group {i+1}:\")\n",
    "            for generation in generation_list:\n",
    "                # ChatGeneration 通常有 'message' 属性，Generation 有 'text'\n",
    "                if hasattr(generation, 'message') and generation.message:\n",
    "                    print(f\"    - Text: {generation.message.content[:100]}...\")\n",
    "                    if generation.message.additional_kwargs:\n",
    "                         print(f\"    - Tool Calls: {generation.message.additional_kwargs.get('tool_calls')}\")\n",
    "                elif hasattr(generation, 'text'):\n",
    "                    print(f\"    - Text: {generation.text[:100]}...\")\n",
    "        if response.llm_output:\n",
    "            print(f\"  LLM Output (Token Usage, etc.): {response.llm_output}\")\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "        print(f\"\\n[{self.description}] LLM Error: {error}\")\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when chain starts running.\"\"\"\n",
    "        print(f\"\\n[{self.description}] Chain Start:\")\n",
    "        print(f\"  Serialized: {serialized}\") # 描述 Chain 的信息\n",
    "        print(f\"  Inputs: {inputs}\")\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when chain ends running.\"\"\"\n",
    "        print(f\"\\n[{self.description}] Chain End:\")\n",
    "        print(f\"  Outputs: {outputs}\")\n",
    "\n",
    "    def on_chain_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when chain errors.\"\"\"\n",
    "        print(f\"\\n[{self.description}] Chain Error: {error}\")\n",
    "\n",
    "    def on_tool_start(\n",
    "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when tool starts running.\"\"\"\n",
    "        print(f\"\\n[{self.description}] Tool Start:\")\n",
    "        print(f\"  Serialized: {serialized}\") # 描述 Tool 的信息\n",
    "        print(f\"  Input: {input_str}\")\n",
    "\n",
    "    def on_tool_end(self, output: str, **kwargs: Any) -> Any: # output 类型可能因工具而异\n",
    "        \"\"\"Run when tool ends running.\"\"\"\n",
    "        print(f\"\\n[{self.description}] Tool End:\")\n",
    "        print(f\"  Output: {output}\")\n",
    "\n",
    "    def on_tool_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when tool errors.\"\"\"\n",
    "        print(f\"\\n[{self.description}] Tool Error: {error}\")\n",
    "\n",
    "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on agent action.\"\"\"\n",
    "        print(f\"\\n[{self.description}] Agent Action:\")\n",
    "        print(f\"  Tool: {action.tool}\")\n",
    "        print(f\"  Tool Input: {action.tool_input}\")\n",
    "        print(f\"  Log: {action.log}\")\n",
    "\n",
    "    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on agent end.\"\"\"\n",
    "        print(f\"\\n[{self.description}] Agent Finish:\")\n",
    "        print(f\"  Return Values: {finish.return_values}\")\n",
    "        print(f\"  Log: {finish.log}\")\n",
    "\n",
    "# 实例化你的自定义处理器\n",
    "my_handler = MyCustomHandler(description=\"LoggerV1\")\n",
    "\n",
    "# 再次运行 LLM，这次使用自定义处理器\n",
    "print(\"\\n--- LLM Call with MyCustomHandler ---\")\n",
    "llm_with_custom_handler  = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    model=\"qwen-plus\",  # 此处以qwen-plus为例，您可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "    temperature=0,\n",
    "    callbacks=[stdout_handler] # 可以传入一个列表\n",
    ")\n",
    "response_custom = llm_with_custom_handler.invoke(\"What are the benefits of learning Langchain?\")\n",
    "# response_custom = llm.invoke(\"What are the benefits of learning Langchain?\", config={\"callbacks\": [my_handler]})\n",
    "print(\"\\nLLM Response (Custom Handler):\")\n",
    "print(response_custom.content)\n",
    "\n",
    "# 使用自定义处理器运行 Chain\n",
    "print(\"\\n--- Chain Call with MyCustomHandler ---\")\n",
    "chain_response_custom = chain.invoke(\n",
    "    {\"country\": \"Canada\"},\n",
    "    config={\"callbacks\": [my_handler]} # 推荐方式\n",
    ")\n",
    "print(\"\\nChain Response (Custom Handler):\")\n",
    "print(chain_response_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Langchain Expression Language (LCEL) 与回调\n",
    "\n",
    "LCEL 是构建 Chain 的现代方式。回调系统与 LCEL 无缝集成，主要通过在 invoke, stream, batch, ainvoke 等方法的 config 参数中传递回调。\n",
    "\n",
    "在 stream 的例子中，streaming_token_handler 会在 LLM 逐个生成 token 时被调用，而 my_handler 则会记录整个 LLM 调用和 Chain 调用的开始和结束事件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LCEL Chain Call with MyCustomHandler ---\n",
      "\n",
      "[LoggerV1] Chain Start:\n",
      "  Serialized: None\n",
      "  Inputs: {'topic': 'the moon'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "[LoggerV1] Chain Start:\n",
      "  Serialized: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['topic'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['topic'], 'template': 'Tell me a fun fact about {topic}.', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}]}, 'name': 'ChatPromptTemplate'}\n",
      "  Inputs: {'topic': 'the moon'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ChatPromptTemplate chain...\u001b[0m\n",
      "\n",
      "[LoggerV1] Chain End:\n",
      "  Outputs: messages=[HumanMessage(content='Tell me a fun fact about the moon.', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "[LoggerV1] Chat Model Start (ID: ecd4b8bc-01d5-4835-89ad-3755fad22f7a):\n",
      "  Message Group 1:\n",
      "    - human: Tell me a fun fact about the moon....\n",
      "\n",
      "[LoggerV1] LLM End (ID: ecd4b8bc-01d5-4835-89ad-3755fad22f7a):\n",
      "  Generation Group 1:\n",
      "    - Text: A fun fact about the Moon is that it has \"moonquakes,\" similar to earthquakes on Earth! These seismi...\n",
      "    - Tool Calls: None\n",
      "  LLM Output (Token Usage, etc.): {'token_usage': {'completion_tokens': 101, 'prompt_tokens': 17, 'total_tokens': 118, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-08d84a0e-983f-9995-94ff-33bd0159480d', 'service_tier': None}\n",
      "\n",
      "[LoggerV1] Chain Start:\n",
      "  Serialized: None\n",
      "  Inputs: content='A fun fact about the Moon is that it has \"moonquakes,\" similar to earthquakes on Earth! These seismic events are caused by the gravitational pull of Earth, as well as by meteorite impacts, thermal expansion, and the contraction of the Moon\\'s interior as it cools. Interestingly, astronauts on the Apollo missions placed seismometers on the lunar surface, which detected thousands of moonquakes. Some of these quakes were strong enough to shift rocks and cause cracks in the Moon\\'s crust!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 17, 'total_tokens': 118, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-08d84a0e-983f-9995-94ff-33bd0159480d', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--ecd4b8bc-01d5-4835-89ad-3755fad22f7a-0' usage_metadata={'input_tokens': 17, 'output_tokens': 101, 'total_tokens': 118, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StrOutputParser chain...\u001b[0m\n",
      "\n",
      "[LoggerV1] Chain End:\n",
      "  Outputs: A fun fact about the Moon is that it has \"moonquakes,\" similar to earthquakes on Earth! These seismic events are caused by the gravitational pull of Earth, as well as by meteorite impacts, thermal expansion, and the contraction of the Moon's interior as it cools. Interestingly, astronauts on the Apollo missions placed seismometers on the lunar surface, which detected thousands of moonquakes. Some of these quakes were strong enough to shift rocks and cause cracks in the Moon's crust!\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "[LoggerV1] Chain End:\n",
      "  Outputs: A fun fact about the Moon is that it has \"moonquakes,\" similar to earthquakes on Earth! These seismic events are caused by the gravitational pull of Earth, as well as by meteorite impacts, thermal expansion, and the contraction of the Moon's interior as it cools. Interestingly, astronauts on the Apollo missions placed seismometers on the lunar surface, which detected thousands of moonquakes. Some of these quakes were strong enough to shift rocks and cause cracks in the Moon's crust!\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "LCEL Chain Response:\n",
      "A fun fact about the Moon is that it has \"moonquakes,\" similar to earthquakes on Earth! These seismic events are caused by the gravitational pull of Earth, as well as by meteorite impacts, thermal expansion, and the contraction of the Moon's interior as it cools. Interestingly, astronauts on the Apollo missions placed seismometers on the lunar surface, which detected thousands of moonquakes. Some of these quakes were strong enough to shift rocks and cause cracks in the Moon's crust!\n",
      "\n",
      "--- LCEL Chain Stream with MyCustomHandler ---\n",
      "\n",
      "[LoggerV1] Chain Start:\n",
      "  Serialized: None\n",
      "  Inputs: {'input': ''}\n",
      "\n",
      "[LoggerV1] Chain Start:\n",
      "  Serialized: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['topic'], 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['topic'], 'template': 'Tell me a fun fact about {topic}.', 'template_format': 'f-string'}, 'name': 'PromptTemplate'}}}]}, 'name': 'ChatPromptTemplate'}\n",
      "  Inputs: {'topic': 'black holes'}\n",
      "\n",
      "[LoggerV1] Chain End:\n",
      "  Outputs: messages=[HumanMessage(content='Tell me a fun fact about black holes.', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "[LoggerV1] Chat Model Start (ID: 7aca20e1-2859-4bde-811c-5b976f56b127):\n",
      "  Message Group 1:\n",
      "    - human: Tell me a fun fact about black holes....\n",
      "Streamed Token: ''\n",
      "[LoggerV1] Chain Start:\n",
      "  Serialized: None\n",
      "  Inputs: {'input': ''}\n",
      "Streamed Token: 'Here'Streamed Token: ''s'Streamed Token: ' a'Streamed Token: ' fun fact about black'Streamed Token: ' holes: **If'Streamed Token: ' you fell into a'Streamed Token: ' black hole, you'Streamed Token: ' would experience something called'Streamed Token: ' \"spaghett'Streamed Token: 'ification.\"** This'Streamed Token: ' is a phenomenon where'Streamed Token: ' the intense gravitational pull'Streamed Token: ' of the black hole'Streamed Token: ' stretches objects (or'Streamed Token: ' people) into long'Streamed Token: ', thin shapes,'Streamed Token: ' much like spaghetti.'Streamed Token: ' As you approach the'Streamed Token: ' black hole, the'Streamed Token: ' gravitational force on your'Streamed Token: ' feet would be much'Streamed Token: ' stronger than on your'Streamed Token: ' head, causing you'Streamed Token: ' to be stretched apart'Streamed Token: '. Of course,'Streamed Token: ' this would be fatal'Streamed Token: ', but it's'Streamed Token: ' a fascinating example of'Streamed Token: ' how extreme gravity can'Streamed Token: ' warp matter!'Streamed Token: ''\n",
      "[LoggerV1] LLM End (ID: 7aca20e1-2859-4bde-811c-5b976f56b127):\n",
      "  Generation Group 1:\n",
      "    - Text: Here's a fun fact about black holes: **If you fell into a black hole, you would experience something...\n",
      "\n",
      "[LoggerV1] Chain End:\n",
      "  Outputs: Here's a fun fact about black holes: **If you fell into a black hole, you would experience something called \"spaghettification.\"** This is a phenomenon where the intense gravitational pull of the black hole stretches objects (or people) into long, thin shapes, much like spaghetti. As you approach the black hole, the gravitational force on your feet would be much stronger than on your head, causing you to be stretched apart. Of course, this would be fatal, but it's a fascinating example of how extreme gravity can warp matter!\n",
      "\n",
      "[LoggerV1] Chain End:\n",
      "  Outputs: Here's a fun fact about black holes: **If you fell into a black hole, you would experience something called \"spaghettification.\"** This is a phenomenon where the intense gravitational pull of the black hole stretches objects (or people) into long, thin shapes, much like spaghetti. As you approach the black hole, the gravitational force on your feet would be much stronger than on your head, causing you to be stretched apart. Of course, this would be fatal, but it's a fascinating example of how extreme gravity can warp matter!\n",
      "\n",
      "\n",
      "LCEL Streamed Full Response:\n",
      "Here's a fun fact about black holes: **If you fell into a black hole, you would experience something called \"spaghettification.\"** This is a phenomenon where the intense gravitational pull of the black hole stretches objects (or people) into long, thin shapes, much like spaghetti. As you approach the black hole, the gravitational force on your feet would be much stronger than on your head, causing you to be stretched apart. Of course, this would be fatal, but it's a fascinating example of how extreme gravity can warp matter!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LCEL Chain\n",
    "prompt_lcel = ChatPromptTemplate.from_template(\"Tell me a fun fact about {topic}.\")\n",
    "\n",
    "\n",
    "# 可以不在这里传入callbacks\n",
    "llm_lcel =  ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    model=\"qwen-plus\",  # 此处以qwen-plus为例，您可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain_lcel = prompt_lcel | llm_lcel | parser\n",
    "\n",
    "print(\"\\n--- LCEL Chain Call with MyCustomHandler ---\")\n",
    "lcel_response = chain_lcel.invoke(\n",
    "    {\"topic\": \"the moon\"},\n",
    "    config={\"callbacks\": [my_handler, stdout_handler]} # 可以传递多个处理器\n",
    ")\n",
    "print(\"\\nLCEL Chain Response:\")\n",
    "print(lcel_response)\n",
    "\n",
    "# 演示 streaming 和 on_llm_new_token\n",
    "print(\"\\n--- LCEL Chain Stream with MyCustomHandler ---\")\n",
    "# 为了看到 on_llm_new_token, 我们需要修改 MyCustomHandler 或使用一个专门的 handler\n",
    "class StreamingTokenHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        print(f\"Streamed Token: '{token}'\", end=\"\", flush=True)\n",
    "\n",
    "streaming_token_handler = StreamingTokenHandler()\n",
    "llm_streaming = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    model=\"qwen-plus\",\n",
    "    temperature=0,\n",
    "    streaming=True, # 启用 streaming\n",
    "    # callbacks=[streaming_token_handler] # 也可以在这里全局设置\n",
    ")\n",
    "chain_streaming = prompt_lcel | llm_streaming | parser\n",
    "\n",
    "full_response_streamed = \"\"\n",
    "for chunk in chain_streaming.stream({\"topic\": \"black holes\"}, config={\"callbacks\": [streaming_token_handler, my_handler]}):\n",
    "    # chunk 是 StrOutputParser 输出的字符串片段\n",
    "    # streaming_token_handler 会在 LLM 层面捕获 token\n",
    "    # my_handler 会捕获 chain 和 llm 的 start/end 事件\n",
    "    full_response_streamed += chunk\n",
    "    # print(f\"Chain Stream Chunk: {chunk}\") # 这是 chain 级别的输出 chunk\n",
    "print(\"\\n\\nLCEL Streamed Full Response:\")\n",
    "print(full_response_streamed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Agent 和 Tool 事件\n",
    "\n",
    "当使用 Agent 和 Tool 时，会有额外的事件如 on_tool_start, on_tool_end, on_agent_action, on_agent_finish。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckduckgo-search\n",
      "  Downloading duckduckgo_search-8.0.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: click>=8.1.8 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from duckduckgo-search) (8.1.8)\n",
      "Collecting primp>=0.15.0 (from duckduckgo-search)\n",
      "  Downloading primp-0.15.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: lxml>=5.3.0 in /Users/zata/miniconda3/envs/test_Langchain/lib/python3.11/site-packages (from duckduckgo-search) (5.4.0)\n",
      "Downloading duckduckgo_search-8.0.1-py3-none-any.whl (18 kB)\n",
      "Downloading primp-0.15.0-cp38-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: primp, duckduckgo-search\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [duckduckgo-search]\n",
      "\u001b[1A\u001b[2KSuccessfully installed duckduckgo-search-8.0.1 primp-0.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/9j2rmds528z0gl5xd89jmdfm0000gn/T/ipykernel_84467/1766976541.py:23: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_executor = initialize_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Agent Call with MyCustomHandler ---\n",
      "\n",
      "[LoggerV1] Chain Start:\n",
      "  Serialized: None\n",
      "  Inputs: {'input': 'What is the latest news about Mars rovers?'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "[LoggerV1] Chain Start:\n",
      "  Serialized: None\n",
      "  Inputs: {'input': 'What is the latest news about Mars rovers?', 'agent_scratchpad': '', 'stop': ['\\nObservation:', '\\n\\tObservation:']}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "DuckDuckGo Search(tool_input: 'Union[str, dict[str, Any]]', verbose: 'Optional[bool]' = None, start_color: 'Optional[str]' = 'green', color: 'Optional[str]' = 'green', callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, config: 'Optional[RunnableConfig]' = None, tool_call_id: 'Optional[str]' = None, **kwargs: 'Any') -> 'Any' - Useful for when you need to answer questions about current events or general knowledge.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [DuckDuckGo Search]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What is the latest news about Mars rovers?\n",
      "Thought:\u001b[0m\n",
      "\n",
      "[LoggerV1] Chat Model Start (ID: 87d8bf45-2ef5-47eb-b816-0c38715fb8e9):\n",
      "  Message Group 1:\n",
      "    - human: Answer the following questions as best you can. You have access to the following...\n",
      "\n",
      "[LoggerV1] LLM End (ID: 87d8bf45-2ef5-47eb-b816-0c38715fb8e9):\n",
      "  Generation Group 1:\n",
      "    - Text: I should use a search engine to find the latest news about Mars rovers.\n",
      "Action: DuckDuckGo Search\n",
      "Ac...\n",
      "    - Tool Calls: None\n",
      "  LLM Output (Token Usage, etc.): {'token_usage': {'completion_tokens': 38, 'prompt_tokens': 295, 'total_tokens': 333, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-ad7106a0-3aa3-95ac-aaa6-a3c7c16f1902', 'service_tier': None}\n",
      "\n",
      "[LoggerV1] Chain End:\n",
      "  Outputs: {'text': 'I should use a search engine to find the latest news about Mars rovers.\\nAction: DuckDuckGo Search\\nAction Input: \"latest news about Mars rovers\"'}\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "[LoggerV1] Agent Action:\n",
      "  Tool: DuckDuckGo Search\n",
      "  Tool Input: latest news about Mars rovers\n",
      "  Log: I should use a search engine to find the latest news about Mars rovers.\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: \"latest news about Mars rovers\"\n",
      "\u001b[32;1m\u001b[1;3mI should use a search engine to find the latest news about Mars rovers.\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: \"latest news about Mars rovers\"\u001b[0m\n",
      "[LoggerV1] Tool Start:\n",
      "  Serialized: {'name': 'DuckDuckGo Search', 'description': 'Useful for when you need to answer questions about current events or general knowledge.'}\n",
      "  Input: latest news about Mars rovers\n",
      "\n",
      "[LoggerV1] Tool Start:\n",
      "  Serialized: {'name': 'duckduckgo_search', 'description': 'A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.'}\n",
      "  Input: latest news about Mars rovers\n",
      "\n",
      "[LoggerV1] Tool End:\n",
      "  Output: NASA's Curiosity Mars rover has never been camera shy, having been seen in selfies and images taken from space. But on Feb. 28 — the 4,466th Martian day, or sol, of the mission — Curiosity was captured in what is believed to be the first orbital image of the rover mid-drive across the Red Planet. ... Latest News. Daily. Weekly. Scientists were stunned on May 30 when a rock that NASA's Curiosity Mars rover drove over cracked open to reveal something never seen before on the Red Planet: yellow sulfur crystals. Since October 2023, the rover has been exploring a region of Mars rich with sulfates, a kind of salt that contains sulfur and forms as water evaporates. But ... One of Perseverance's hazard cameras captured the rover's coring drill collecting the \"Main River\" rock sample on \"Witch Hazel Hill\" on March 10, 2025, the 1,441st Martian day, or sol, of the ... NASA's Mars rover Curiosity captured this image of its current workspace, containing well-preserved polygonal shaped fractures, with waffle or honeycomb patterns. Get the latest news releases, features, findings, and stories about the missions on Mars. Get the latest news releases, features, findings, and stories about the missions on Mars. ... New findings from NASA's Curiosity Mars rover could provide an answer to the mystery of what happened to the planet's ancient atmosphere and how Mars has ...\n",
      "\u001b[32;1m\u001b[1;3mNASA's Curiosity Mars rover has never been camera shy, having been seen in selfies and images taken from space. But on Feb. 28 — the 4,466th Martian day, or sol, of the mission — Curiosity was captured in what is believed to be the first orbital image of the rover mid-drive across the Red Planet. ... Latest News. Daily. Weekly. Scientists were stunned on May 30 when a rock that NASA's Curiosity Mars rover drove over cracked open to reveal something never seen before on the Red Planet: yellow sulfur crystals. Since October 2023, the rover has been exploring a region of Mars rich with sulfates, a kind of salt that contains sulfur and forms as water evaporates. But ... One of Perseverance's hazard cameras captured the rover's coring drill collecting the \"Main River\" rock sample on \"Witch Hazel Hill\" on March 10, 2025, the 1,441st Martian day, or sol, of the ... NASA's Mars rover Curiosity captured this image of its current workspace, containing well-preserved polygonal shaped fractures, with waffle or honeycomb patterns. Get the latest news releases, features, findings, and stories about the missions on Mars. Get the latest news releases, features, findings, and stories about the missions on Mars. ... New findings from NASA's Curiosity Mars rover could provide an answer to the mystery of what happened to the planet's ancient atmosphere and how Mars has ...\u001b[0m\n",
      "[LoggerV1] Tool End:\n",
      "  Output: NASA's Curiosity Mars rover has never been camera shy, having been seen in selfies and images taken from space. But on Feb. 28 — the 4,466th Martian day, or sol, of the mission — Curiosity was captured in what is believed to be the first orbital image of the rover mid-drive across the Red Planet. ... Latest News. Daily. Weekly. Scientists were stunned on May 30 when a rock that NASA's Curiosity Mars rover drove over cracked open to reveal something never seen before on the Red Planet: yellow sulfur crystals. Since October 2023, the rover has been exploring a region of Mars rich with sulfates, a kind of salt that contains sulfur and forms as water evaporates. But ... One of Perseverance's hazard cameras captured the rover's coring drill collecting the \"Main River\" rock sample on \"Witch Hazel Hill\" on March 10, 2025, the 1,441st Martian day, or sol, of the ... NASA's Mars rover Curiosity captured this image of its current workspace, containing well-preserved polygonal shaped fractures, with waffle or honeycomb patterns. Get the latest news releases, features, findings, and stories about the missions on Mars. Get the latest news releases, features, findings, and stories about the missions on Mars. ... New findings from NASA's Curiosity Mars rover could provide an answer to the mystery of what happened to the planet's ancient atmosphere and how Mars has ...\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mNASA's Curiosity Mars rover has never been camera shy, having been seen in selfies and images taken from space. But on Feb. 28 — the 4,466th Martian day, or sol, of the mission — Curiosity was captured in what is believed to be the first orbital image of the rover mid-drive across the Red Planet. ... Latest News. Daily. Weekly. Scientists were stunned on May 30 when a rock that NASA's Curiosity Mars rover drove over cracked open to reveal something never seen before on the Red Planet: yellow sulfur crystals. Since October 2023, the rover has been exploring a region of Mars rich with sulfates, a kind of salt that contains sulfur and forms as water evaporates. But ... One of Perseverance's hazard cameras captured the rover's coring drill collecting the \"Main River\" rock sample on \"Witch Hazel Hill\" on March 10, 2025, the 1,441st Martian day, or sol, of the ... NASA's Mars rover Curiosity captured this image of its current workspace, containing well-preserved polygonal shaped fractures, with waffle or honeycomb patterns. Get the latest news releases, features, findings, and stories about the missions on Mars. Get the latest news releases, features, findings, and stories about the missions on Mars. ... New findings from NASA's Curiosity Mars rover could provide an answer to the mystery of what happened to the planet's ancient atmosphere and how Mars has ...\u001b[0m\n",
      "Thought:\n",
      "[LoggerV1] Chain Start:\n",
      "  Serialized: None\n",
      "  Inputs: {'input': 'What is the latest news about Mars rovers?', 'agent_scratchpad': 'I should use a search engine to find the latest news about Mars rovers.\\nAction: DuckDuckGo Search\\nAction Input: \"latest news about Mars rovers\"\\nObservation: NASA\\'s Curiosity Mars rover has never been camera shy, having been seen in selfies and images taken from space. But on Feb. 28 — the 4,466th Martian day, or sol, of the mission — Curiosity was captured in what is believed to be the first orbital image of the rover mid-drive across the Red Planet. ... Latest News. Daily. Weekly. Scientists were stunned on May 30 when a rock that NASA\\'s Curiosity Mars rover drove over cracked open to reveal something never seen before on the Red Planet: yellow sulfur crystals. Since October 2023, the rover has been exploring a region of Mars rich with sulfates, a kind of salt that contains sulfur and forms as water evaporates. But ... One of Perseverance\\'s hazard cameras captured the rover\\'s coring drill collecting the \"Main River\" rock sample on \"Witch Hazel Hill\" on March 10, 2025, the 1,441st Martian day, or sol, of the ... NASA\\'s Mars rover Curiosity captured this image of its current workspace, containing well-preserved polygonal shaped fractures, with waffle or honeycomb patterns. Get the latest news releases, features, findings, and stories about the missions on Mars. Get the latest news releases, features, findings, and stories about the missions on Mars. ... New findings from NASA\\'s Curiosity Mars rover could provide an answer to the mystery of what happened to the planet\\'s ancient atmosphere and how Mars has ...\\nThought:', 'stop': ['\\nObservation:', '\\n\\tObservation:']}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "DuckDuckGo Search(tool_input: 'Union[str, dict[str, Any]]', verbose: 'Optional[bool]' = None, start_color: 'Optional[str]' = 'green', color: 'Optional[str]' = 'green', callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, config: 'Optional[RunnableConfig]' = None, tool_call_id: 'Optional[str]' = None, **kwargs: 'Any') -> 'Any' - Useful for when you need to answer questions about current events or general knowledge.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [DuckDuckGo Search]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What is the latest news about Mars rovers?\n",
      "Thought:I should use a search engine to find the latest news about Mars rovers.\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: \"latest news about Mars rovers\"\n",
      "Observation: NASA's Curiosity Mars rover has never been camera shy, having been seen in selfies and images taken from space. But on Feb. 28 — the 4,466th Martian day, or sol, of the mission — Curiosity was captured in what is believed to be the first orbital image of the rover mid-drive across the Red Planet. ... Latest News. Daily. Weekly. Scientists were stunned on May 30 when a rock that NASA's Curiosity Mars rover drove over cracked open to reveal something never seen before on the Red Planet: yellow sulfur crystals. Since October 2023, the rover has been exploring a region of Mars rich with sulfates, a kind of salt that contains sulfur and forms as water evaporates. But ... One of Perseverance's hazard cameras captured the rover's coring drill collecting the \"Main River\" rock sample on \"Witch Hazel Hill\" on March 10, 2025, the 1,441st Martian day, or sol, of the ... NASA's Mars rover Curiosity captured this image of its current workspace, containing well-preserved polygonal shaped fractures, with waffle or honeycomb patterns. Get the latest news releases, features, findings, and stories about the missions on Mars. Get the latest news releases, features, findings, and stories about the missions on Mars. ... New findings from NASA's Curiosity Mars rover could provide an answer to the mystery of what happened to the planet's ancient atmosphere and how Mars has ...\n",
      "Thought:\u001b[0m\n",
      "\n",
      "[LoggerV1] Chat Model Start (ID: daa86d04-5485-4077-8297-a376a9bc4f6f):\n",
      "  Message Group 1:\n",
      "    - human: Answer the following questions as best you can. You have access to the following...\n",
      "\n",
      "[LoggerV1] LLM End (ID: daa86d04-5485-4077-8297-a376a9bc4f6f):\n",
      "  Generation Group 1:\n",
      "    - Text: I now know the final answer.\n",
      "Final Answer: The latest news about Mars rovers includes NASA's Curiosi...\n",
      "    - Tool Calls: None\n",
      "  LLM Output (Token Usage, etc.): {'token_usage': {'completion_tokens': 182, 'prompt_tokens': 648, 'total_tokens': 830, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-18c9c171-3fdf-96c5-a3c7-7d37e2b53019', 'service_tier': None}\n",
      "\n",
      "[LoggerV1] Chain End:\n",
      "  Outputs: {'text': 'I now know the final answer.\\nFinal Answer: The latest news about Mars rovers includes NASA\\'s Curiosity Mars rover being captured in what is believed to be the first orbital image mid-drive across the Red Planet. Additionally, on May 30, a rock that Curiosity drove over cracked open to reveal yellow sulfur crystals, which have never been seen before on Mars. Since October 2023, Curiosity has been exploring a region of Mars rich with sulfates. Meanwhile, Perseverance\\'s hazard cameras captured the rover\\'s coring drill collecting the \"Main River\" rock sample on \"Witch Hazel Hill\" on March 10, 2025. Curiosity also captured an image of its current workspace, containing well-preserved polygonal shaped fractures with waffle or honeycomb patterns. These findings contribute to understanding Mars\\' ancient atmosphere and the planet\\'s history.'}\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "[LoggerV1] Agent Finish:\n",
      "  Return Values: {'output': 'The latest news about Mars rovers includes NASA\\'s Curiosity Mars rover being captured in what is believed to be the first orbital image mid-drive across the Red Planet. Additionally, on May 30, a rock that Curiosity drove over cracked open to reveal yellow sulfur crystals, which have never been seen before on Mars. Since October 2023, Curiosity has been exploring a region of Mars rich with sulfates. Meanwhile, Perseverance\\'s hazard cameras captured the rover\\'s coring drill collecting the \"Main River\" rock sample on \"Witch Hazel Hill\" on March 10, 2025. Curiosity also captured an image of its current workspace, containing well-preserved polygonal shaped fractures with waffle or honeycomb patterns. These findings contribute to understanding Mars\\' ancient atmosphere and the planet\\'s history.'}\n",
      "  Log: I now know the final answer.\n",
      "Final Answer: The latest news about Mars rovers includes NASA's Curiosity Mars rover being captured in what is believed to be the first orbital image mid-drive across the Red Planet. Additionally, on May 30, a rock that Curiosity drove over cracked open to reveal yellow sulfur crystals, which have never been seen before on Mars. Since October 2023, Curiosity has been exploring a region of Mars rich with sulfates. Meanwhile, Perseverance's hazard cameras captured the rover's coring drill collecting the \"Main River\" rock sample on \"Witch Hazel Hill\" on March 10, 2025. Curiosity also captured an image of its current workspace, containing well-preserved polygonal shaped fractures with waffle or honeycomb patterns. These findings contribute to understanding Mars' ancient atmosphere and the planet's history.\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: The latest news about Mars rovers includes NASA's Curiosity Mars rover being captured in what is believed to be the first orbital image mid-drive across the Red Planet. Additionally, on May 30, a rock that Curiosity drove over cracked open to reveal yellow sulfur crystals, which have never been seen before on Mars. Since October 2023, Curiosity has been exploring a region of Mars rich with sulfates. Meanwhile, Perseverance's hazard cameras captured the rover's coring drill collecting the \"Main River\" rock sample on \"Witch Hazel Hill\" on March 10, 2025. Curiosity also captured an image of its current workspace, containing well-preserved polygonal shaped fractures with waffle or honeycomb patterns. These findings contribute to understanding Mars' ancient atmosphere and the planet's history.\u001b[0m\n",
      "\n",
      "[LoggerV1] Chain End:\n",
      "  Outputs: {'output': 'The latest news about Mars rovers includes NASA\\'s Curiosity Mars rover being captured in what is believed to be the first orbital image mid-drive across the Red Planet. Additionally, on May 30, a rock that Curiosity drove over cracked open to reveal yellow sulfur crystals, which have never been seen before on Mars. Since October 2023, Curiosity has been exploring a region of Mars rich with sulfates. Meanwhile, Perseverance\\'s hazard cameras captured the rover\\'s coring drill collecting the \"Main River\" rock sample on \"Witch Hazel Hill\" on March 10, 2025. Curiosity also captured an image of its current workspace, containing well-preserved polygonal shaped fractures with waffle or honeycomb patterns. These findings contribute to understanding Mars\\' ancient atmosphere and the planet\\'s history.'}\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Agent Final Response:\n",
      "The latest news about Mars rovers includes NASA's Curiosity Mars rover being captured in what is believed to be the first orbital image mid-drive across the Red Planet. Additionally, on May 30, a rock that Curiosity drove over cracked open to reveal yellow sulfur crystals, which have never been seen before on Mars. Since October 2023, Curiosity has been exploring a region of Mars rich with sulfates. Meanwhile, Perseverance's hazard cameras captured the rover's coring drill collecting the \"Main River\" rock sample on \"Witch Hazel Hill\" on March 10, 2025. Curiosity also captured an image of its current workspace, containing well-preserved polygonal shaped fractures with waffle or honeycomb patterns. These findings contribute to understanding Mars' ancient atmosphere and the planet's history.\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# 定义一个简单的工具\n",
    "search_tool = Tool(\n",
    "    name=\"DuckDuckGo Search\",\n",
    "    func=DuckDuckGoSearchRun().run,\n",
    "    description=\"Useful for when you need to answer questions about current events or general knowledge.\",\n",
    ")\n",
    "\n",
    "# 初始化 Agent (使用旧版 agent executor 来清晰演示)\n",
    "# 注意：新版 agents 推荐使用 `create_openai_functions_agent` 等配合 `AgentExecutor`\n",
    "# 但回调机制是通用的\n",
    "try:\n",
    "    # 使用一个支持函数调用的模型\n",
    "    agent_llm = ChatOpenAI(\n",
    "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        model=\"qwen-plus\",  # 此处以qwen-plus为例，您可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "        temperature=0,\n",
    "    )\n",
    "    # 旧版 initialize_agent 仍然可以工作，但请注意 Langchain 的发展方向\n",
    "    # verbose=True 本身也使用了一个 Callback Handler\n",
    "    agent_executor = initialize_agent(\n",
    "        tools=[search_tool],\n",
    "        llm=agent_llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, # 简单的 Agent 类型\n",
    "        verbose=False, # 设置为 False 避免与我们的 Handler 输出混淆\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Agent Call with MyCustomHandler ---\")\n",
    "    agent_response = agent_executor.invoke(\n",
    "        {\"input\": \"What is the latest news about Mars rovers?\"},\n",
    "        config={\"callbacks\": [my_handler, stdout_handler]} # 使用我们的自定义处理器和 stdout\n",
    "    )\n",
    "    print(\"\\nAgent Final Response:\")\n",
    "    print(agent_response[\"output\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing or running agent: {e}\")\n",
    "    print(\"Skipping agent example. Ensure you have 'duckduckgo-search' installed if using DuckDuckGoSearchRun.\")\n",
    "    print(\"pip install duckduckgo-search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 异步回调处理器 (AsyncCallbackHandler)\n",
    "\n",
    "如果你的应用主要使用 Langchain 的异步功能（如 ainvoke, astream），你应该使用 AsyncCallbackHandler。方法签名将是 async def。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MyAsyncHandler] Async Handler Initialized\n",
      "\n",
      "--- (Async) LLM Call with MyAsyncCustomHandler ---\n",
      "\n",
      "[MyAsyncHandler] (Async) LLM Start:\n",
      "  Prompts: ['Human: Tell me about asynchronous programming.']\n",
      "\n",
      "[MyAsyncHandler] (Async) LLM End:\n",
      "\n",
      "Async LLM Response:\n",
      "Asynchronous programming is a technique used in software development to allow programs to perform multiple operations concurrently without blocking the execution of other tasks. This approach is particularly useful when dealing with I/O-bound or network-bound operations, where tasks may involve waiting for external resources such as databases, file systems, or web services.\n",
      "\n",
      "### Key Concepts of Asynchronous Programming\n",
      "\n",
      "1. **Blocking vs Non-Blocking**:\n",
      "   - In synchronous (blocking) programming, the program waits for an operation to complete before moving on to the next task. For example, if you're reading data from a file, the program will pause until the read operation finishes.\n",
      "   - In asynchronous (non-blocking) programming, the program can continue executing other tasks while waiting for the result of an operation. Once the operation completes, the program is notified and can handle the result.\n",
      "\n",
      "2. **Concurrency**:\n",
      "   - Asynchronous programming enables concurrency, which means that multiple tasks can be in progress at the same time, even if they are not executed in parallel (i.e., true parallelism). Concurrency is achieved by interleaving tasks, allowing the program to make progress on multiple fronts without waiting for each task to complete.\n",
      "\n",
      "3. **Event Loops**:\n",
      "   - Many asynchronous systems use an event loop to manage tasks. The event loop continuously checks for tasks that are ready to be executed and schedules them accordingly. When a task is waiting for an external resource, the event loop can switch to another task, ensuring efficient use of CPU time.\n",
      "\n",
      "4. **Callbacks, Promises, and Async/Await**:\n",
      "   - **Callbacks**: A callback is a function passed as an argument to another function, which is executed once the operation completes. While simple, callbacks can lead to \"callback hell\" when dealing with nested or chained operations.\n",
      "   - **Promises**: A Promise represents the eventual completion (or failure) of an asynchronous operation and its resulting value. Promises provide a cleaner way to handle asynchronous operations compared to callbacks.\n",
      "   - **Async/Await**: This is a syntactic sugar built on top of Promises, making asynchronous code look more like synchronous code. It allows you to write asynchronous code in a more linear and readable manner.\n",
      "\n",
      "### Benefits of Asynchronous Programming\n",
      "\n",
      "1. **Improved Performance**:\n",
      "   - By avoiding blocking calls, asynchronous programming allows applications to handle more tasks simultaneously, improving overall throughput and responsiveness.\n",
      "\n",
      "2. **Better Resource Utilization**:\n",
      "   - Since the program doesn't wait idly for I/O operations to complete, it can utilize CPU cycles more effectively by working on other tasks.\n",
      "\n",
      "3. **Scalability**:\n",
      "   - Asynchronous programming is particularly beneficial in server-side applications, where handling many concurrent connections efficiently is crucial. Frameworks like Node.js leverage asynchronous programming to achieve high scalability.\n",
      "\n",
      "### Challenges of Asynchronous Programming\n",
      "\n",
      "1. **Complexity**:\n",
      "   - Writing and reasoning about asynchronous code can be more complex than synchronous code, especially when dealing with error handling, race conditions, and ordering of operations.\n",
      "\n",
      "2. **Debugging**:\n",
      "   - Debugging asynchronous code can be more challenging because the flow of execution is not always linear, and errors may occur at unpredictable times.\n",
      "\n",
      "3. **Learning Curve**:\n",
      "   - Developers need to understand concepts like event loops, promises, and async/await to effectively write and maintain asynchronous code.\n",
      "\n",
      "### Example in JavaScript (Using Async/Await)\n",
      "\n",
      "Here’s a simple example of asynchronous programming using `async` and `await` in JavaScript:\n",
      "\n",
      "```javascript\n",
      "// Simulate an asynchronous API call\n",
      "function fetchData() {\n",
      "  return new Promise((resolve) => {\n",
      "    setTimeout(() => resolve(\"Data from API\"), 2000); // Simulate a 2-second delay\n",
      "  });\n",
      "}\n",
      "\n",
      "async function getData() {\n",
      "  console.log(\"Fetching data...\");\n",
      "  const data = await fetchData(); // Wait for the promise to resolve\n",
      "  console.log(data);\n",
      "  console.log(\"Done!\");\n",
      "}\n",
      "\n",
      "getData();\n",
      "```\n",
      "\n",
      "In this example:\n",
      "- The `fetchData` function simulates an asynchronous API call using a `Promise`.\n",
      "- The `getData` function uses `async` and `await` to wait for the `fetchData` promise to resolve before continuing execution.\n",
      "- The program does not block during the 2-second wait, allowing other tasks to run in the meantime.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Asynchronous programming is essential for building responsive and scalable applications, especially in environments where I/O operations are frequent. While it introduces some complexity, modern tools and language features like `async/await` make it easier to write and maintain asynchronous code. Understanding asynchronous programming is crucial for developers working on web applications, microservices, and other performance-critical systems.\n",
      "\n",
      "--- (Async) LCEL Chain Call with MyAsyncCustomHandler ---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ChatPromptTemplate chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "[MyAsyncHandler] (Async) LLM Start:\n",
      "  Prompts: ['Human: Tell me a fun fact about quantum computing.']\n",
      "\n",
      "[MyAsyncHandler] (Async) LLM End:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StrOutputParser chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Async LCEL Chain Response:\n",
      "Sure! Here's a fun fact about quantum computing:\n",
      "\n",
      "Quantum computers use **qubits** instead of classical bits, and one fascinating property of qubits is **quantum superposition**. This means a qubit can exist not just as a 0 or a 1 (like in classical computing), but as a combination of both 0 and 1 at the same time! \n",
      "\n",
      "Because of this, a quantum computer with just 300 qubits could theoretically process more numbers simultaneously than there are atoms in the observable universe. That’s because the number of possible states grows exponentially with each additional qubit — for *n* qubits, you can represent $2^n$ states at once. This exponential power is what makes quantum computers so potentially revolutionary for solving certain complex problems that would take classical computers millions of years to solve!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks.base import AsyncCallbackHandler\n",
    "\n",
    "class MyAsyncCustomHandler(AsyncCallbackHandler):\n",
    "    def __init__(self, description: str = \"MyAsyncHandler\"):\n",
    "        self.description = description\n",
    "        print(f\"[{self.description}] Async Handler Initialized\")\n",
    "\n",
    "    async def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> None:\n",
    "        print(f\"\\n[{self.description}] (Async) LLM Start:\")\n",
    "        # print(f\"  Serialized: {serialized}\")\n",
    "        print(f\"  Prompts: {prompts}\")\n",
    "\n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        print(f\"\\n[{self.description}] (Async) LLM End:\")\n",
    "        # print(f\"  Response: {response.generations[0][0].text[:80]}...\")\n",
    "\n",
    "    # ... 你可以实现其他 on_... 方法的异步版本\n",
    "    # 例如: async def on_chain_start(...) etc.\n",
    "\n",
    "# 实例化异步处理器\n",
    "my_async_handler = MyAsyncCustomHandler()\n",
    "\n",
    "# 使用异步处理器运行 LLM\n",
    "print(\"\\n--- (Async) LLM Call with MyAsyncCustomHandler ---\")\n",
    "# llm_async = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, callbacks=[my_async_handler]) # 不推荐这样用于异步\n",
    "# response_async = await llm_async.ainvoke(\"Tell me about asynchronous programming.\")\n",
    "\n",
    "# 推荐使用 config\n",
    "llm_for_async = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    model=\"qwen-plus\",  # 此处以qwen-plus为例，您可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "    temperature=0,\n",
    ")\n",
    "response_async = await llm_for_async.ainvoke(\n",
    "    \"Tell me about asynchronous programming.\",\n",
    "    config={\"callbacks\": [my_async_handler, stdout_handler]}\n",
    ")\n",
    "print(\"\\nAsync LLM Response:\")\n",
    "print(response_async.content)\n",
    "\n",
    "# 异步 LCEL Chain\n",
    "print(\"\\n--- (Async) LCEL Chain Call with MyAsyncCustomHandler ---\")\n",
    "chain_lcel_async = prompt_lcel | llm_for_async | parser\n",
    "async_lcel_response = await chain_lcel_async.ainvoke(\n",
    "    {\"topic\": \"quantum computing\"},\n",
    "    config={\"callbacks\": [my_async_handler, stdout_handler]}\n",
    ")\n",
    "print(\"\\nAsync LCEL Chain Response:\")\n",
    "print(async_lcel_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要运行包含 await 的异步代码，你需要在一个异步上下文中执行它，例如在一个 async def main(): 函数中，然后用 asyncio.run(main()) 来运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MyAsyncHandler] Async Handler Initialized\n",
      "\n",
      "--- (Async) LLM Call with MyAsyncCustomHandler ---\n",
      "\n",
      "[MyAsyncHandler] (Async) LLM Start:\n",
      "  Prompts: ['Human: Tell me about asynchronous programming.']\n",
      "\n",
      "[MyAsyncHandler] (Async) LLM End:\n",
      "\n",
      "Async LLM Response:\n",
      "Asynchronous programming is a technique that allows a program to perform other tasks while waiting for certain operations to complete. This approach contrasts with synchronous programming, where the program execution halts and waits until the current operation finishes before moving on to the next one.\n",
      "\n",
      "### Key Concepts of Asynchronous Programming:\n",
      "\n",
      "1. **Non-Blocking Operations**: In asynchronous programming, when an operation (such as reading from a file, making a network request, or querying a database) starts, the program doesn't wait for it to finish. Instead, it continues executing other parts of the code, improving efficiency and responsiveness.\n",
      "\n",
      "2. **Concurrency**: Asynchronous programming enables concurrency, meaning multiple tasks can be in progress at the same time without blocking each other. However, this is not the same as parallelism, which involves executing tasks simultaneously on multiple processors or cores.\n",
      "\n",
      "3. **Event-Driven Architecture**: Many asynchronous systems are based on event-driven architectures. When an asynchronous operation completes, it triggers an event or callback, allowing the program to handle the result.\n",
      "\n",
      "4. **Callbacks, Promises, and Async/Await**:\n",
      "   - **Callbacks**: A callback is a function passed into another function as an argument, which is executed once the operation completes.\n",
      "   - **Promises**: A Promise represents the eventual completion (or failure) of an asynchronous operation and its resulting value. It provides a cleaner way to handle asynchronous results compared to callbacks.\n",
      "   - **Async/Await**: This is syntactic sugar built on top of Promises, allowing you to write asynchronous code that looks more like synchronous code, making it easier to read and maintain.\n",
      "\n",
      "### Why Use Asynchronous Programming?\n",
      "\n",
      "- **Improved Performance**: By avoiding blocking operations, programs can make better use of system resources, especially in I/O-bound applications (e.g., web servers handling many simultaneous requests).\n",
      "  \n",
      "- **Responsiveness**: Applications remain responsive even when performing long-running tasks. For example, a user interface can stay interactive while loading data in the background.\n",
      "\n",
      "- **Scalability**: Asynchronous programming is particularly useful in high-concurrency environments, such as web servers, where many clients may be waiting for responses simultaneously.\n",
      "\n",
      "### Common Use Cases\n",
      "\n",
      "- **Web Development**: Handling HTTP requests and responses, interacting with databases, and managing WebSocket connections often benefit from asynchronous programming.\n",
      "  \n",
      "- **File I/O**: Reading or writing large files without blocking the main thread is a common use case.\n",
      "  \n",
      "- **Networking**: Making network calls, such as fetching data from APIs, can be done asynchronously to prevent the application from freezing during network delays.\n",
      "\n",
      "### Example in JavaScript (using `async/await`):\n",
      "\n",
      "```javascript\n",
      "// Synchronous example (blocks execution)\n",
      "function fetchDataSync() {\n",
      "    const data = require('fs').readFileSync('file.txt', 'utf8');\n",
      "    console.log(data);\n",
      "}\n",
      "\n",
      "// Asynchronous example (non-blocking)\n",
      "async function fetchDataAsync() {\n",
      "    try {\n",
      "        const data = await require('fs').promises.readFile('file.txt', 'utf8');\n",
      "        console.log(data);\n",
      "    } catch (err) {\n",
      "        console.error(err);\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "In the synchronous version, the program will pause until the file is fully read. In the asynchronous version, the program continues running while waiting for the file to be read, only pausing briefly when it needs the result.\n",
      "\n",
      "### Challenges in Asynchronous Programming\n",
      "\n",
      "- **Complexity**: Managing asynchronous flows can introduce complexity, especially when dealing with nested callbacks (often referred to as \"callback hell\").\n",
      "  \n",
      "- **Error Handling**: Errors in asynchronous code need special handling to ensure they don't go unnoticed. Promises and `async/await` help mitigate some of these issues.\n",
      "\n",
      "- **Debugging**: Debugging asynchronous code can be more challenging because the flow of execution is non-linear.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Asynchronous programming is essential for building efficient, responsive, and scalable applications, particularly in environments where I/O operations dominate. Modern programming languages and frameworks provide robust tools (like Promises and async/await) to simplify asynchronous code, reducing complexity and improving maintainability.\n",
      "\n",
      "--- (Async) LCEL Chain Call with MyAsyncCustomHandler ---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RunnableSequence chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ChatPromptTemplate chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "[MyAsyncHandler] (Async) LLM Start:\n",
      "  Prompts: ['Human: Explain black holes in simple terms.']\n",
      "\n",
      "[MyAsyncHandler] (Async) LLM End:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StrOutputParser chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Async LCEL Chain Response:\n",
      "Sure! Let's break it down in simple terms:\n",
      "\n",
      "A **black hole** is a region in space where the pull of gravity is so strong that nothing, not even light, can escape from it. Here's how they form and why they're special:\n",
      "\n",
      "1. **How They Form**: Black holes usually form when a very massive star runs out of fuel and collapses under its own weight. The core of the star gets squeezed into an extremely small and dense point called a \"singularity.\"\n",
      "\n",
      "2. **The Event Horizon**: Around the singularity is a boundary called the **event horizon**. This is like the \"point of no return.\" Anything that crosses this boundary, including light, gets pulled in and can't come back out.\n",
      "\n",
      "3. **Why They're Invisible**: Since light can't escape, black holes don't shine or emit light, making them invisible to our eyes. Scientists detect them by observing how they affect nearby stars and gas.\n",
      "\n",
      "4. ** Sizes of Black Holes**: There are different types of black holes:\n",
      "   - **Stellar black holes**: These are formed from stars and are a few times more massive than our Sun.\n",
      "   - **Supermassive black holes**: These are huge, with masses equal to millions or billions of Suns, and are found at the centers of galaxies, including our own Milky Way.\n",
      "\n",
      "In short, black holes are regions in space with gravity so intense that nothing can escape, and they play a big role in shaping the universe!\n",
      "\n",
      "--- (Async) LCEL Chain Stream with MyAsyncCustomHandler ---\n",
      "\n",
      "[MyAsyncHandler] (Async) LLM Start:\n",
      "  Prompts: ['Human: Explain neural networks in simple terms.']\n",
      "Async Streamed Token: ''Async Streamed Token: 'Sure'Async Streamed Token: '!'Async Streamed Token: ' Let'Async Streamed Token: ''s break it down'Async Streamed Token: ' in simple terms.\n",
      "\n",
      "'Async Streamed Token: 'A **neural'Async Streamed Token: ' network** is a'Async Streamed Token: ' type of computer system'Async Streamed Token: ' inspired by the human'Async Streamed Token: ' brain. It's'Async Streamed Token: ' designed to recognize patterns'Async Streamed Token: ', learn from examples'Async Streamed Token: ', and make decisions'Async Streamed Token: ', much like how'Async Streamed Token: ' we humans process information'Async Streamed Token: '.\n",
      "\n",
      "### Imagine this'Async Streamed Token: ':\n",
      "1. **'Async Streamed Token: 'Neurons**: In'Async Streamed Token: ' the brain, neurons'Async Streamed Token: ' are the tiny cells'Async Streamed Token: ' that send and receive'Async Streamed Token: ' signals. In a'Async Streamed Token: ' neural network, we'Async Streamed Token: ' have \"artificial'Async Streamed Token: ' neurons\" that do'Async Streamed Token: ' similar work — they'Async Streamed Token: ' take in information,'Async Streamed Token: ' process it, and'Async Streamed Token: ' pass it along.\n",
      "\n",
      "'Async Streamed Token: '2. **Layers'Async Streamed Token: '**: Neural networks are'Async Streamed Token: ' organized into layers.'Async Streamed Token: ' There’s an **'Async Streamed Token: 'input layer** ('Async Streamed Token: 'where data goes in'Async Streamed Token: '), one or more'Async Streamed Token: ' **hidden layers**'Async Streamed Token: ' (where the processing'Async Streamed Token: ' happens), and an'Async Streamed Token: ' **output layer**'Async Streamed Token: ' (where the result'Async Streamed Token: ' comes out).\n",
      "\n",
      "3'Async Streamed Token: '. **Learning**:'Async Streamed Token: ' When you train a'Async Streamed Token: ' neural network, you'Async Streamed Token: ' give it lots of'Async Streamed Token: ' examples (like pictures'Async Streamed Token: ' of cats and dogs'Async Streamed Token: ') and tell it'Async Streamed Token: ' what the correct answers'Async Streamed Token: ' are. Over time'Async Streamed Token: ', it adjusts its'Async Streamed Token: ' internal settings (called'Async Streamed Token: ' \"weights\") to'Async Streamed Token: ' get better at recognizing'Async Streamed Token: ' patterns and making predictions'Async Streamed Token: '.\n",
      "\n",
      "4. **'Async Streamed Token: 'Predictions**: Once'Async Streamed Token: ' trained, the neural'Async Streamed Token: ' network can look at'Async Streamed Token: ' new data (like'Async Streamed Token: ' a picture it hasn'Async Streamed Token: '’t seen before)'Async Streamed Token: ' and try to predict'Async Streamed Token: ' what it is ('Async Streamed Token: 'e.g., \"'Async Streamed Token: 'this is a cat'Async Streamed Token: '\").\n",
      "\n",
      "### A Simple'Async Streamed Token: ' Example:\n",
      "Think of'Async Streamed Token: ' teaching a child to'Async Streamed Token: ' recognize apples. You'Async Streamed Token: ' show them many apples'Async Streamed Token: ' and say, \"'Async Streamed Token: 'This is an apple'Async Streamed Token: '.\" Gradually,'Async Streamed Token: ' they learn the features'Async Streamed Token: ' of an apple:'Async Streamed Token: ' its color, shape'Async Streamed Token: ', size, etc'Async Streamed Token: '. A neural network'Async Streamed Token: ' does something similar.'Async Streamed Token: ' You feed it lots'Async Streamed Token: ' of apple pictures and'Async Streamed Token: ' tell it which ones'Async Streamed Token: ' are apples. Eventually'Async Streamed Token: ', it learns to'Async Streamed Token: ' identify apples on its'Async Streamed Token: ' own.\n",
      "\n",
      "In short'Async Streamed Token: ', neural networks are'Async Streamed Token: ' powerful tools that help'Async Streamed Token: ' computers learn from data'Async Streamed Token: ' and make smart decisions'Async Streamed Token: ', just like humans'Async Streamed Token: ' do!'Async Streamed Token: ''\n",
      "[MyAsyncHandler] (Async) LLM End:\n",
      "\n",
      "\n",
      "Async LCEL Streamed Full Response:\n",
      "Sure! Let's break it down in simple terms.\n",
      "\n",
      "A **neural network** is a type of computer system inspired by the human brain. It's designed to recognize patterns, learn from examples, and make decisions, much like how we humans process information.\n",
      "\n",
      "### Imagine this:\n",
      "1. **Neurons**: In the brain, neurons are the tiny cells that send and receive signals. In a neural network, we have \"artificial neurons\" that do similar work — they take in information, process it, and pass it along.\n",
      "\n",
      "2. **Layers**: Neural networks are organized into layers. There’s an **input layer** (where data goes in), one or more **hidden layers** (where the processing happens), and an **output layer** (where the result comes out).\n",
      "\n",
      "3. **Learning**: When you train a neural network, you give it lots of examples (like pictures of cats and dogs) and tell it what the correct answers are. Over time, it adjusts its internal settings (called \"weights\") to get better at recognizing patterns and making predictions.\n",
      "\n",
      "4. **Predictions**: Once trained, the neural network can look at new data (like a picture it hasn’t seen before) and try to predict what it is (e.g., \"this is a cat\").\n",
      "\n",
      "### A Simple Example:\n",
      "Think of teaching a child to recognize apples. You show them many apples and say, \"This is an apple.\" Gradually, they learn the features of an apple: its color, shape, size, etc. A neural network does something similar. You feed it lots of apple pictures and tell it which ones are apples. Eventually, it learns to identify apples on its own.\n",
      "\n",
      "In short, neural networks are powerful tools that help computers learn from data and make smart decisions, just like humans do!\n",
      "An error occurred: asyncio.run() cannot be called from a running event loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/9j2rmds528z0gl5xd89jmdfm0000gn/T/ipykernel_84467/1776738706.py:90: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  print(f\"An error occurred: {e}\")\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    class MyAsyncCustomHandler(AsyncCallbackHandler):\n",
    "        def __init__(self, description: str = \"MyAsyncHandler\"):\n",
    "            self.description = description\n",
    "            print(f\"[{self.description}] Async Handler Initialized\")\n",
    "\n",
    "        async def on_llm_start(\n",
    "            self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "        ) -> None:\n",
    "            print(f\"\\n[{self.description}] (Async) LLM Start:\")\n",
    "            # print(f\"  Serialized: {serialized}\")\n",
    "            print(f\"  Prompts: {prompts}\")\n",
    "\n",
    "        async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "            print(f\"\\n[{self.description}] (Async) LLM End:\")\n",
    "\n",
    "    # 实例化异步处理器\n",
    "    my_async_handler = MyAsyncCustomHandler()\n",
    "    stdout_async_handler = StdOutCallbackHandler() # StdOutCallbackHandler 也能在异步场景下工作\n",
    "\n",
    "    # 使用异步处理器运行 LLM\n",
    "    print(\"\\n--- (Async) LLM Call with MyAsyncCustomHandler ---\")\n",
    "    llm_for_async = ChatOpenAI(\n",
    "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        model=\"qwen-plus\",  # 此处以qwen-plus为例，您可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "        temperature=0,\n",
    "    )\n",
    "    response_async = await llm_for_async.ainvoke(\n",
    "        \"Tell me about asynchronous programming.\",\n",
    "        config={\"callbacks\": [my_async_handler, stdout_async_handler]}\n",
    "    )\n",
    "    print(\"\\nAsync LLM Response:\")\n",
    "    print(response_async.content)\n",
    "\n",
    "    # 异步 LCEL Chain\n",
    "    print(\"\\n--- (Async) LCEL Chain Call with MyAsyncCustomHandler ---\")\n",
    "    prompt_lcel_async = ChatPromptTemplate.from_template(\"Explain {concept} in simple terms.\")\n",
    "    chain_lcel_async = prompt_lcel_async | llm_for_async | StrOutputParser()\n",
    "    async_lcel_response = await chain_lcel_async.ainvoke(\n",
    "        {\"concept\": \"black holes\"},\n",
    "        config={\"callbacks\": [my_async_handler, stdout_async_handler]}\n",
    "    )\n",
    "    print(\"\\nAsync LCEL Chain Response:\")\n",
    "    print(async_lcel_response)\n",
    "\n",
    "    print(\"\\n--- (Async) LCEL Chain Stream with MyAsyncCustomHandler ---\")\n",
    "    # 为了看到 on_llm_new_token, 我们需要一个异步的 streaming handler\n",
    "    class AsyncStreamingTokenHandler(AsyncCallbackHandler):\n",
    "        async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "            print(f\"Async Streamed Token: '{token}'\", end=\"\", flush=True)\n",
    "\n",
    "    async_streaming_token_handler = AsyncStreamingTokenHandler()\n",
    "    llm_async_streaming = ChatOpenAI(\n",
    "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        model=\"qwen-plus\",  # 此处以qwen-plus为例，您可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "        temperature=0,\n",
    "        streaming=True,\n",
    "    )\n",
    "    chain_async_streaming = prompt_lcel_async | llm_async_streaming | StrOutputParser()\n",
    "\n",
    "    full_async_response_streamed = \"\"\n",
    "    async for chunk in chain_async_streaming.astream(\n",
    "        {\"concept\": \"neural networks\"},\n",
    "        config={\"callbacks\": [async_streaming_token_handler, my_async_handler]}\n",
    "    ):\n",
    "        full_async_response_streamed += chunk\n",
    "    print(\"\\n\\nAsync LCEL Streamed Full Response:\")\n",
    "    print(full_async_response_streamed)\n",
    "\n",
    "\n",
    "# 如果你在 Jupyter Notebook 或类似环境中，可以直接 await main()\n",
    "await main()\n",
    "# 否则，使用 asyncio.run()\n",
    "if __name__ == \"__main__\":\n",
    "    # 注意：直接在脚本顶层使用 await 需要 Python 3.8+ 的交互式解释器或特定运行方式\n",
    "    # 在普通 .py 文件中，标准做法是：\n",
    "    try:\n",
    "        asyncio.run(main())\n",
    "    except OSError as e:\n",
    "        if \"Cannot run the event loop while another loop is running\" in str(e):\n",
    "            print(\"\\nSkipping asyncio.run(main()) as an event loop is already running (e.g., in Jupyter).\")\n",
    "            print(\"You can typically 'await main()' directly in such environments.\")\n",
    "        else:\n",
    "            raise e\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. aevents() (实验性/较新特性)\n",
    "\n",
    "Langchain 引入了 aevents API，它提供了一种通过异步生成器流式传输事件的更现代方式。这允许你以编程方式迭代运行过程中的事件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (确保在异步上下文中运行，例如上面的 main() 函数)\n",
    "\n",
    "async def demonstrate_aevents():\n",
    "    print(\"\\n--- Demonstrating aevents API ---\")\n",
    "    llm = ChatOpenAI(\n",
    "        api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        model=\"qwen-plus\",  # 此处以qwen-plus为例，您可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "        temperature=0,\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_template(\"What are three key features of Python?\")\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # aevents 返回一个异步生成器\n",
    "    event_stream = chain.aevents(\n",
    "        {\"input\": \"What are three key features of Python?\"}, # LCEL chain.invoke takes a dict\n",
    "        # 如果你的 chain 输入 key 不是 \"input\", 请相应修改\n",
    "        # 例如, 如果 prompt_lcel = ChatPromptTemplate.from_template(\"Tell me a fun fact about {topic}.\")\n",
    "        # 则 chain.aevents({\"topic\": \"the moon\"})\n",
    "        config={\"tags\": [\"aevents_demo\"]} # 可以添加标签\n",
    "    )\n",
    "\n",
    "    async for event in event_stream:\n",
    "        event_name = event[\"name\"] # 事件的名称，如 on_llm_start\n",
    "        run_id = event[\"run_id\"]   # 运行 ID\n",
    "        print(f\"Event: {event_name} (Run ID: {run_id})\")\n",
    "        # print(f\"  Data: {event['data']}\") # 事件携带的数据\n",
    "        if event_name == \"on_chat_model_stream\": # 对于 streaming\n",
    "            print(f\"    Chunk: {event['data']['chunk']}\")\n",
    "        elif event_name == \"on_chat_model_end\":\n",
    "            if event['data'].get('output') and hasattr(event['data']['output'], 'generations'):\n",
    "                 print(f\"    LLM Output: {event['data']['output'].generations[0][0].message.content[:50]}...\")\n",
    "        # 你可以根据 event_name 检查不同的事件类型并处理其 data\n",
    "        # 例如 event[\"name\"] 可以是 \"on_llm_start\", \"on_llm_stream\", \"on_llm_end\",\n",
    "        # \"on_chain_start\", \"on_chain_end\", etc.\n",
    "        # event[\"data\"] 包含该事件的具体信息\n",
    "\n",
    "# 在 main 函数中调用:\n",
    "# await demonstrate_aevents()\n",
    "# 然后运行 asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 demonstrate_aevents 添加到 main 函数中并运行，你将看到每个事件被异步地打印出来。aevents 非常强大，因为它允许你以编程方式访问和处理事件流，而不仅仅是通过回调函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LangSmith 和事件\n",
    "\n",
    "LangSmith 是 Langchain 的官方可观测性和调试平台。当你配置好 LangSmith（通常通过设置环境变量 LANGCHAIN_API_KEY, LANGCHAIN_TRACING_V2=\"true\", LANGCHAIN_PROJECT）后，Langchain 会自动将所有这些内部事件（回调数据）发送到 LangSmith 服务器。\n",
    "\n",
    "你不需要显式地添加回调处理器来与 LangSmith 集成（除非你想在发送到 LangSmith 之前 进行某些自定义本地处理）。LangSmith 内部就利用了这个事件系统来捕获所有运行的详细信息。\n",
    "\n",
    "总结\n",
    "\n",
    "Langchain 的事件/回调系统是其核心功能之一，提供了强大的可观测性和灵活性。\n",
    "\n",
    "使用内置的 StdOutCallbackHandler 进行快速调试。\n",
    "创建自定义的 BaseCallbackHandler 或 AsyncCallbackHandler 来实现特定的日志记录、监控或交互逻辑。\n",
    "通过 config={\"callbacks\": [...]} 参数将处理器传递给 LLMs, Chains (特别是 LCEL), 和 Agents 的 invoke, stream, batch (以及对应的 ainvoke, astream, abatch) 方法。\n",
    "利用 aevents() API 以异步流的方式消费事件。\n",
    "理解 LangSmith 在后台利用此事件系统来提供丰富的追踪和调试体验。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_Langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
